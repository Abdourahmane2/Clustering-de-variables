% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CAH.R
\name{CAH}
\alias{CAH}
\title{Hierarchical Clustering on Variables}
\value{
An R6 object of class CAH. The object is mutable and methods modify it in-place.
Most methods return \code{self} invisibly to allow method chaining:

\code{cah$fit(data)$cutree(k = 3)$print()}

The \code{cutree()} method additionally returns the cluster assignment vector.
}
\description{
An R6 class for performing hierarchical clustering on variables using
correlation-based distances. This implementation uses Ward's method and
provides tools for partitioning variables into clusters, creating latent
components via PCA, and assigning new variables to existing clusters.
}
\details{
Hierarchical Clustering on Variables (CAH)

The CAH class implements variable clustering with the following features:
\itemize{
\item Automatic data validation and cleaning
\item Correlation-based distance matrix
\item Ward's hierarchical clustering method (ward.D2)
\item Automatic optimal k detection using elbow method
\item PCA-based latent components for each cluster
\item Prediction of new variable assignments
\item Quality metrics (R², Silhouette, n²)
\item Multiple visualization types
}

The algorithm groups variables that are highly correlated, allowing for
dimensionality reduction and better interpretation of complex datasets.
Each cluster is represented by a latent component (first principal component)
that captures the common information shared by variables in that cluster.
}
\note{
\strong{Important considerations:}
\itemize{
\item The method works best with at least 4 variables for stable results
\item All variables are automatically standardized within each cluster before PCA
\item Missing values are removed using complete cases (listwise deletion)
\item Constant variables (zero variance) are automatically excluded
\item Qualitative variables (factors, characters) are automatically excluded
\item The correlation-based distance is scale-invariant
\item For large datasets (>1000 variables), consider using a subset or
pre-filtering correlated variables
\item The number of clusters k must be between 1 and (number of variables - 1)
\item Supplementary variables (from predict()) do not affect the clustering
structure; they are only assigned to existing clusters for illustration
}

\strong{Interpretation guidelines:}
\itemize{
\item Variables in the same cluster are highly correlated and measure similar concepts
\item The latent component (Zk) represents the common information shared by
variables in a cluster
\item Use \code{summary()} to identify the most representative variable (parangon)
in each cluster - this is the variable with the highest squared correlation
with the latent component
\item The parangon can be used as a single representative variable for the entire cluster
\item Squared correlations (R²) in the summary indicate how well each variable is
represented by the latent component (values close to 1 are better)
\item The average correlation (\code{corr_moy}) gives an overall measure of
redundancy in the dataset
\item R² > 0.70 indicates good clustering; R² > 0.50 is acceptable
\item Mean Silhouette > 0.6 indicates excellent structure; > 0.4 is acceptable
\item η² > 0.8 for a variable indicates excellent representation in its cluster
}

\strong{Computational complexity:}
\itemize{
\item Time: O(n²p + p²n) where n = individuals, p = variables
\item Space: O(p²) for correlation and distance matrices
\item The bottleneck is typically the correlation matrix computation for large n
}
}
\section{Methods}{

\describe{
\item{\code{new(method = "ward.D2")}}{
Initialize a new CAH object.
\itemize{
\item \code{method}: Aggregation method for hierarchical clustering
(default: "ward.D2")
}
}

\item{\code{fit(data)}}{
Fit the hierarchical clustering model on the provided data.

\strong{Parameters:}
\itemize{
\item \code{data}: A data.frame or matrix with at least 2 individuals
(rows) and 2 variables (columns)
}

\strong{The method performs:}
\enumerate{
\item Data validation and type checking
\item Removal of qualitative variables (factors, characters)
\item Removal of constant variables (zero variance)
\item Handling of missing values (complete cases only)
\item Computation of correlation matrix
\item Creation of correlation-based distance matrix
\item Hierarchical clustering using Ward's method
\item Automatic detection of optimal k using elbow method
}

\strong{Returns:} Self (invisibly) for method chaining
}

\item{\code{cutree(k = NULL)}}{
Cut the dendrogram to create k clusters and compute latent components.

\strong{Parameters:}
\itemize{
\item \code{k}: Number of clusters (if NULL, uses automatically
detected best_k). Must be between 1 and (number of variables - 1).
}

\strong{Returns:} Named integer vector of cluster assignments
For each cluster, a latent component is created using PCA on the
standardized variables within that cluster. The latent component
represents the first principal axis and captures the maximum variance.
Also computes quality metrics : R², Silhouette scores, and n² (eta-squared)
}

\item{\code{predict(X_new)}}{
Assign new variables to existing clusters based on correlation with
latent components.

\strong{Parameters:}
\itemize{
\item \code{X_new}: A data.frame with the same number of rows as the
*original data. Must not contain missing values.
}
\strong{Returns:} Self (invisibly)
New variables are standardized and assigned to the cluster whose latent
component has the highest absolute correlation. The assignment is stored
in the \code{predict_result} field. These are supplementary variables,
which means they did not participate in cluster formation.
}

\item{\code{print(...)}}{
Print a concise summary of the CAH object showing:
\itemize{
\item Data dimensions
\item Clustering method
\item Average correlation
\item Optimal k
\item Cluster sizes
\item Number of supplementary variables
\item BSS ratio and GAP statistics for cluster quality
\item Variable membership
}
}

\item{\code{summary(...)}}{
Print a detailed summary including:
\itemize{
\item Model information (method, correlation, optimal k)
\item Distribution of variables per cluster
\item List of variables in each cluster
\item Local PCA results (squared correlations with latent component)
\item Representative variable (parangon) for each cluster
\item Supplementary variables and their assignments
\item Complete partition (active + supplementary variables)
}
}
\item{\code{plot(type = "dendrogramme")}}{
Create visualizations of the CAH results.

\if{html}{\out{<div class="sourceCode">}}\preformatted{\\strong\{Parameters:\}
}\if{html}{\out{</div>}}

\itemize{
\item \code{type}: Type of plot to generate. Options are:
\itemize{
\item \code{"dendrogramme"}: Hierarchical clustering dendrogram with
cluster rectangles (default)
\item \code{"acp"}: PCA biplot showing variable correlations
\item \code{"mds"}: Multidimensional scaling projection
\item \code{"silhouette"}: Silhouette plot for cluster quality
\item \code{"elbow"}: Elbow method plot for optimal k detection
}
}
}
}
}

\section{Mathematical Details}{


\strong{Distance calculation:}

The distance between two variables \eqn{X_i} and \eqn{X_j} is computed as:

\deqn{d(X_i, X_j) = \sqrt{2(1 - |r_{ij}|)}}

where \eqn{r_{ij}} is the Pearson correlation coefficient between \eqn{X_i}
and \eqn{X_j}. This distance ranges from 0 (perfect correlation) to
\eqn{\sqrt{2}} (perfect anti-correlation or independence).

\strong{Ward's criterion:}

Ward's method minimizes the within-cluster variance. The distance between
two clusters \eqn{C_i} and \eqn{C_j} is:

\deqn{\Delta(C_i, C_j) = \frac{n_i n_j}{n_i + n_j} ||m_i - m_j||^2}

where \eqn{n_i} and \eqn{n_j} are cluster sizes, and \eqn{m_i} and \eqn{m_j}
are cluster centroids in the variable space.

\strong{Optimal k detection:}

The optimal number of clusters is detected using a variation of the elbow
method applied to the dendrogram heights. Instead of computing curvature,
the algorithm identifies the largest jump between consecutive merge heights.

\deqn{k_{opt} = \arg\max_{i} \Delta h_i + 1}

where \eqn{\Delta h_i = h_{i+1} - h_i} represents the discrete first-order
difference between successive heights. A large value of \eqn{\Delta h_i}
indicates a natural separation between clusters. The optimal number of
clusters is thus selected just before the largest increase in height.

\strong{Latent component:}

For each cluster k, the latent component \eqn{Z_k} is the first principal
component obtained by PCA on the standardized variables within the cluster:

\deqn{Z_k = \sum_{j \in C_k} a_j X_j^*}

where \eqn{X_j^*} are the standardized variables and \eqn{a_j} are the
loadings maximizing the explained variance. The standardization is performed
locally within each cluster to ensure comparability.

\strong{Variable assignment (prediction):}

A new variable \eqn{X_{new}} is assigned to the cluster k that maximizes:

\deqn{k^* = \arg\max_k |r(X_{new}^*, Z_k)|}

where \eqn{X_{new}^*} is the standardized new variable and \eqn{r} denotes
the Pearson correlation coefficient.

\strong{Quality Metrics:}

\enumerate{
\item \strong{R² (Coefficient of Determination):} Measures the proportion
of variance explained by the clustering. Computed as the mean of η²
values across all variables. Ranges from 0 to 1; higher values indicate
better cluster cohesion.

\item \strong{Silhouette Score:} Measures how similar a variable is to its own
cluster compared to other clusters. For variable i in cluster C:

\if{html}{\out{<div class="sourceCode">}}\preformatted{    \deqn{s(i) = \frac{b(i) - a(i)}{\max(a(i), b(i))}}

    where a(i) is mean distance to variables in same cluster and
    b(i) is minimum mean distance to variables in other clusters.
    Ranges from -1 to 1; values > 0.6 indicate excellent clustering.
}\if{html}{\out{</div>}}

\item \strong{η² (Eta-squared):} Represents the squared correlation between
a variable and its cluster's latent component. Indicates how well
a variable is represented by the cluster. Ranges from 0 to 1.
}

\strong{Cluster Quality Indices:}

\enumerate{
\item \strong{BSS Ratio:} Between-cluster sum of squares divided by total
sum of squares. Higher values indicate more distinct clusters.

\item \strong{GAP statistic:} Measures the jump in BSS ratio at each k.
The optimal k typically corresponds to the largest GAP value.
}
}

\section{Public Fields}{

\describe{
\item{\code{data}}{Data.frame containing the cleaned input data (quantitative
variables only, no missing values)}
\item{\code{X_last}}{Data.frame of the last variables used in predict().
Used for displaying correlations in summary()}
\item{\code{method}}{Character string of aggregation method (default: "ward.D2")}
\item{\code{dist_method}}{Character string of distance method (always "correlation")}
\item{\code{corr_moy}}{Numeric value of average absolute correlation between
all pairs of variables. Ranges from 0 (no correlation) to 1 (perfect correlation)}
\item{\code{dist_matrix}}{Distance matrix of class "dist" computed from correlations}
\item{\code{hc}}{Hierarchical clustering object of class "hclust". Contains the
dendrogram structure and can be plotted with plot()}
\item{\code{best_k}}{Integer, optimal number of clusters detected automatically
using the elbow method}
\item{\code{clusters}}{Named integer vector of cluster assignments for each
variable. Names are variable names, values are cluster numbers (1 to k)}
\item{\code{predict_result}}{Named integer vector of cluster assignments for
new variables added via predict()}
\item{\code{compo_latent}}{List of latent components, one per cluster. Each
element contains:
\itemize{
\item \code{Zk}: Numeric vector of principal component scores (length = n individuals)
\item \code{vars}: Character vector of variable names in the cluster
\item \code{cor_vals}: Numeric vector of correlations between variables and Zk
\item \code{scaled_data}: Matrix of standardized data for variables in the cluster
}
}
\item{\code{r2_info}}{List containing R² information:
\itemize{
\item \code{r_squared}: Numeric, mean η² across all variables
\item \code{percentage}: Numeric, R² expressed as percentage
}
}
\item{\code{silhouette}}{List containing silhouette analysis results:
\itemize{
\item \code{scores}: Numeric vector of silhouette scores per variable
\item \code{mean_score}: Numeric, mean silhouette score for all variables
}
}
\item{\code{eta2}}{Named numeric vector of η² values for each variable,
sorted in descending order. Represents squared correlation with latent
component. Higher values indicate variables better represented by their cluster}
}
}

\examples{
# ═══════════════════════════════════════════════════════════
# Example 1: Basic usage with synthetic data
# ═══════════════════════════════════════════════════════════
set.seed(123)
df <- data.frame(
  var1 = rnorm(30, 10, 2),   # Group 1
  var2 = rnorm(30, 12, 2),   # Group 1 (correlated with var1)
  var3 = rnorm(30, 50, 5),   # Group 2
  var4 = rnorm(30, 52, 5)    # Group 2 (correlated with var3)
)

# Initialize and fit
cah <- CAH$new()
cah$fit(df)

# View optimal k
cat("Optimal number of clusters:", cah$best_k, "\n")

# Partition into clusters
cah$cutree(k = 2)

# View concise results
cah$print()

# View detailed results
cah$summary()

# ═══════════════════════════════════════════════════════════
# Example 2: Adding supplementary variables
# ═══════════════════════════════════════════════════════════

# Create new variables similar to existing ones
new_vars <- data.frame(
  var5 = df$var1 * 0.9 + rnorm(30, 0, 0.5),  # Similar to var1
  var6 = df$var3 * 0.9 + rnorm(30, 0, 1)     # Similar to var3
)

# Assign to existing clusters
cah$predict(new_vars)

# View assignments
print(cah$predict_result)

# ═══════════════════════════════════════════════════════════
# Example 3: Quality metrics interpretation
# ═══════════════════════════════════════════════════════════

# After calling cutree(), access quality metrics:
cat("R² =", round(cah$r2_info$r_squared, 4), "\n")
cat("Mean Silhouette =", round(cah$silhouette$mean_score, 4), "\n")

# View η² for each variable (quality of representation)
print(cah$eta2)


# ═══════════════════════════════════════════════════════════
# Example 4: Method chaining
# ═══════════════════════════════════════════════════════════
\dontrun{
CAH$new()$fit(df)$cutree()$print()
}

# ═══════════════════════════════════════════════════════════
# Example 5: Working with real data (mtcars)
# ═══════════════════════════════════════════════════════════
\dontrun{
data(mtcars)

# Select numeric variables
df_cars <- mtcars[, c("mpg", "disp", "hp", "drat", "wt", "qsec")]

# Fit and cluster
cah_cars <- CAH$new()
cah_cars$fit(df_cars)
cah_cars$cutree()  # Uses optimal k automatically

# Visualize dendrogram
plot(cah_cars$hc, main = "Vehicle Variables Clustering",
     xlab = "Variables", ylab = "Distance")
rect.hclust(cah_cars$hc, k = cah_cars$best_k, border = "red")

# Identify representative variables
cah_cars$summary()

# Extract cluster memberships
clusters <- cah_cars$clusters
print(clusters)

# Get latent components for further analysis
for (i in seq_along(cah_cars$compo_latent)) {
  comp <- cah_cars$compo_latent[[i]]
  cat("\nCluster", i, ":\n")
  cat("  Variables:", paste(comp$vars, collapse = ", "), "\n")
  cat("  Variance explained:", round(var(comp$Zk), 2), "\n")
}
}

# ═══════════════════════════════════════════════════════════
# Example 5: Handling different data quality issues
# ═══════════════════════════════════════════════════════════
\dontrun{
# Data with missing values
df_na <- data.frame(
  var1 = c(rnorm(25), rep(NA, 5)),
  var2 = rnorm(30),
  var3 = rnorm(30)
)

cah_na <- CAH$new()
cah_na$fit(df_na)  # Will remove rows with NA (with warning)

# Data with constant variables
df_const <- data.frame(
  var1 = rnorm(30),
  var2 = rep(5, 30),  # Constant - will be removed
  var3 = rnorm(30)
)

cah_const <- CAH$new()
cah_const$fit(df_const)  # Will remove constant variable (with warning)

# Data with qualitative variables
df_mixed <- data.frame(
  var1 = rnorm(30),
  var2 = rnorm(30),
  category = factor(rep(c("A", "B", "C"), 10))  # Will be removed
)

cah_mixed <- CAH$new()
cah_mixed$fit(df_mixed)  # Will remove qualitative variable (with warning)
}

# ═══════════════════════════════════════════════════════════
# Example 6: Visualizing clustering results
# ═══════════════════════════════════════════════════════════
\dontrun{
cah <- CAH$new()
cah$fit(df)
cah$cutree(k = 2)

# Create all visualization types
cah$plot("dendrogramme")  # Dendrogram with cluster rectangles
cah$plot("acp")           # PCA biplot
cah$plot("mds")           # MDS projection
cah$plot("silhouette")    # Silhouette plot
cah$plot("elbow")         # Elbow method
}

# ═══════════════════════════════════════════════════════════
# Example 7: Interpretation workflow for Shiny integration
# ═══════════════════════════════════════════════════════════
\dontrun{
# This workflow is ideal for Shiny applications where users
# can select active vs supplementary variables

# Step 1: Fit on active variables
cah <- CAH$new()
active_vars <- df[, c("var1", "var2", "var3")]
cah$fit(active_vars)

# Step 2: Choose optimal k
cah$cutree(k = cah$best_k)

# Step 3: Print summary
cah$summary()

# Step 4: Add supplementary variables for illustration
supp_vars <- df[, c("var4", "var5")]
cah$predict(supp_vars)

# Step 5: Final detailed summary with both
cah$summary()

# Step 6: Visualize
cah$plot("dendrogramme")
cah$plot("silhouette")
}

}
\references{
Rakotomalala, R. (2020). \emph{Classification ascendante hiérarchique}.
Université Lyon 2.
\url{https://eric.univ-lyon2.fr/ricco/cours/cours_classification_hierarchique.html}

Rakotomalala, R. (2020). \emph{Caractérisation des classes en classification automatique}.
Université Lyon 2.

Rakotomalala, R. (2019). \emph{Les classes R6 sous R (Programmation orientée objet sous R)}.
Université Lyon 2.

Rakotomalala, R. \emph{Tanagra - Hierarchical Agglomerative Clustering with PCA}.
\url{http://tutoriels-data-mining.blogspot.com/}

Husson, F., Lê, S., & Pagès, J. (2017). \emph{Exploratory Multivariate Analysis by Example Using R}
(2nd ed.). Chapman and Hall/CRC.

#' Kaufman, L., & Rousseeuw, P. J. (2005). \emph{Finding Groups in Data: An Introduction to
Cluster Analysis}. John Wiley & Sons.
}
\seealso{
\strong{Related R functions:}
\itemize{
\item \code{\link[stats]{hclust}} - Hierarchical clustering
\item \code{\link[stats]{cutree}} - Cut dendrogram into groups
\item \code{\link[stats]{prcomp}} - Principal component analysis
\item \code{\link[stats]{cor}} - Correlation matrix
\item \code{\link[stats]{dist}} - Distance matrix computation
\item \code{\link[stats]{cmdscale}} - Multidimensional scaling
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{data}}{Data.frame containing the cleaned input data}

\item{\code{X_last}}{Data.frame of last variables used in predict()}

\item{\code{method}}{Character string of aggregation method (ward.D2)}

\item{\code{dist_method}}{Distance method for variable clustering}

\item{\code{corr_moy}}{Numeric, average correlation between variables}

\item{\code{hc}}{Hierarchical clustering object (class "hclust")}

\item{\code{best_k}}{Integer, optimal number of clusters}

\item{\code{clusters}}{Named integer vector of cluster assignments}

\item{\code{predict_result}}{Named integer vector of new variable assignments}

\item{\code{dist_matrix}}{Distance matrix (dissimularity)}

\item{\code{compo_latent}}{List of latent components per cluster}

\item{\code{k_current}}{Integer, current k used for cutree}

\item{\code{r2_info}}{List containing R² metrics for each cluster and global R².}

\item{\code{silhouette}}{Numeric vector or list containing silhouette values}

\item{\code{eta2}}{Named numeric vector containing η² (eta-squared) values}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-CAH-new}{\code{CAH$new()}}
\item \href{#method-CAH-fit}{\code{CAH$fit()}}
\item \href{#method-CAH-cutree}{\code{CAH$cutree()}}
\item \href{#method-CAH-predict}{\code{CAH$predict()}}
\item \href{#method-CAH-plot}{\code{CAH$plot()}}
\item \href{#method-CAH-print}{\code{CAH$print()}}
\item \href{#method-CAH-summary}{\code{CAH$summary()}}
\item \href{#method-CAH-clone}{\code{CAH$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-new"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-new}{}}}
\subsection{Method \code{new()}}{
Create a new CAH object (Constructor)
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$new(method = "ward.D2")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{Character string specifying the agglomeration method for
hierarchical clustering. Default is "ward.D2"}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new \code{CAH} object
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-fit"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-fit}{}}}
\subsection{Method \code{fit()}}{
Fit the hierarchical clustering model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$fit(data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{A data.frame or matrix with at least 2 rows and 2 columns}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-cutree"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-cutree}{}}}
\subsection{Method \code{cutree()}}{
Cut the dendrogram to create clusters
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$cutree(k = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{k}}{Integer, number of clusters. If NULL, uses best_k}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Named integer vector of cluster assignments
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-predict"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-predict}{}}}
\subsection{Method \code{predict()}}{
Predict cluster assignments for new variables
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$predict(X_new)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_new}}{A data.frame with the same number of rows as the original data.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-plot"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-plot}{}}}
\subsection{Method \code{plot()}}{
Plot various visualizations of the CAH model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$plot(type = "dendrogramme")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{type}}{Character string specifying plot type}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-print"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-print}{}}}
\subsection{Method \code{print()}}{
Print a summary of the CAH object
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$print(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments (ignored)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-summary"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-summary}{}}}
\subsection{Method \code{summary()}}{
Print a detailed summary of the CAH model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$summary(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments (ignored)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-clone"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
