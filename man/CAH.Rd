% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CAH.R
\name{CAH}
\alias{CAH}
\title{Hierarchical Clustering on Variables}
\value{
An R6 object of class CAH. The object is mutable and methods modify it in-place.
Most methods return \code{self} invisibly to allow method chaining:

\code{cah$fit(data)$cutree(k = 3)$print()}

The \code{cutree()} method additionally returns the cluster assignment vector.
}
\description{
An R6 class for performing hierarchical clustering on variables using
correlation-based distances. This implementation uses Ward's method and
provides tools for partitioning variables into clusters, creating latent
components via PCA, and assigning new variables to existing clusters.
}
\details{
Hierarchical Clustering on Variables (CAH)

The CAH class implements variable clustering with the following features:
\itemize{
\item Automatic data validation and cleaning
\item Correlation-based distance matrix
\item Ward's hierarchical clustering method (ward.D2)
\item Automatic optimal k detection using elbow method
\item PCA-based latent components for each cluster
\item Prediction of new variable assignments
}

The algorithm groups variables that are highly correlated, allowing for
dimensionality reduction and better interpretation of complex datasets.
Each cluster is represented by a latent component (first principal component)
that captures the common information shared by variables in that cluster.
}
\note{
\strong{Important considerations:}
\itemize{
\item The method works best with at least 4 variables for stable results
\item All variables are automatically standardized within each cluster before PCA
\item Missing values are removed using complete cases (listwise deletion)
\item Constant variables (zero variance) are automatically excluded
\item Qualitative variables (factors, characters) are automatically excluded
\item The correlation-based distance is scale-invariant
\item For large datasets (>1000 variables), consider using a subset or
pre-filtering correlated variables
\item The number of clusters k must be between 1 and (number of variables - 1)
}

\strong{Interpretation guidelines:}
\itemize{
\item Variables in the same cluster are highly correlated and measure similar concepts
\item The latent component (Zk) represents the common information shared by
variables in a cluster
\item Use \code{summary()} to identify the most representative variable (parangon)
in each cluster - this is the variable with the highest squared correlation
with the latent component
\item The parangon can be used as a single representative variable for the entire cluster
\item Squared correlations (R²) in the summary indicate how well each variable is
represented by the latent component (values close to 1 are better)
\item The average correlation (\code{corr_moy}) gives an overall measure of
redundancy in the dataset
}

\strong{Computational complexity:}
\itemize{
\item Time: O(n²p + p²n) where n = individuals, p = variables
\item Space: O(p²) for correlation and distance matrices
\item The bottleneck is typically the correlation matrix computation for large n
}
}
\section{Methods}{

\describe{
\item{\code{new(method = "ward.D2")}}{
Initialize a new CAH object.
\itemize{
\item \code{method}: Aggregation method for hierarchical clustering
(default: "ward.D2")
}
}

\item{\code{fit(data)}}{
Fit the hierarchical clustering model on the provided data.

\strong{Parameters:}
\itemize{
\item \code{data}: A data.frame or matrix with at least 2 individuals
(rows) and 2 variables (columns)
}

\strong{The method performs:}
\enumerate{
\item Data validation and type checking
\item Removal of qualitative variables (factors, characters)
\item Removal of constant variables (zero variance)
\item Handling of missing values (complete cases only)
\item Computation of correlation matrix
\item Creation of correlation-based distance matrix
\item Hierarchical clustering using Ward's method
\item Automatic detection of optimal k using elbow method
}

\strong{Returns:} Self (invisibly) for method chaining
}

\item{\code{cutree(k = NULL)}}{
Cut the dendrogram to create k clusters and compute latent components.

\strong{Parameters:}
\itemize{
\item \code{k}: Number of clusters (if NULL, uses automatically
detected best_k). Must be between 1 and (number of variables - 1).
}

\strong{Returns:} Named integer vector of cluster assignments
For each cluster, a latent component is created using PCA on the
standardized variables within that cluster. The latent component
represents the first principal axis and captures the maximum variance.
}

\item{\code{predict(X_new)}}{
Assign new variables to existing clusters based on correlation with
latent components.

\strong{Parameters:}
\itemize{
\item \code{X_new}: A data.frame with the same number of rows as the
*original data. Must not contain missing values.
}
\strong{Returns:} Self (invisibly)
New variables are standardized and assigned to the cluster whose latent
component has the highest absolute correlation. The assignment is stored
in the \code{predict_result} field.
}

\item{\code{print(...)}}{
Print a concise summary of the CAH object showing:
\itemize{
\item Data dimensions
\item Clustering method
\item Average correlation
\item Optimal k
\item Cluster sizes
\item Number of supplementary variables
}
}

\item{\code{summary(...)}}{
Print a detailed summary including:
\itemize{
\item Model information (method, correlation, optimal k)
\item Distribution of variables per cluster
\item List of variables in each cluster
\item Local PCA results (squared correlations with latent component)
\item Representative variable (parangon) for each cluster
\item Supplementary variables and their assignments
\item Complete partition (active + supplementary variables)
}
}
}
}

\section{Mathematical Details}{


\strong{Distance calculation:}

The distance between two variables \eqn{X_i} and \eqn{X_j} is computed as:

\deqn{d(X_i, X_j) = \sqrt{2(1 - |r_{ij}|)}}

where \eqn{r_{ij}} is the Pearson correlation coefficient between \eqn{X_i}
and \eqn{X_j}. This distance ranges from 0 (perfect correlation) to
\eqn{\sqrt{2}} (perfect anti-correlation or independence).

\strong{Ward's criterion:}

Ward's method minimizes the within-cluster variance. The distance between
two clusters \eqn{C_i} and \eqn{C_j} is:

\deqn{\Delta(C_i, C_j) = \frac{n_i n_j}{n_i + n_j} ||m_i - m_j||^2}

where \eqn{n_i} and \eqn{n_j} are cluster sizes, and \eqn{m_i} and \eqn{m_j}
are cluster centroids in the variable space.

\strong{Optimal k detection:}

The optimal number of clusters is detected using the elbow method on the
dendrogram heights. The algorithm identifies the point of maximum curvature
by computing the second derivative of the height sequence:

\deqn{k_{opt} = \arg\min_{i} \Delta^2 h_i}

where \eqn{\Delta^2 h_i = (h_{i+1} - h_i) - (h_i - h_{i-1})} is the
discrete second derivative.

\strong{Latent component:}

For each cluster k, the latent component \eqn{Z_k} is the first principal
component obtained by PCA on the standardized variables within the cluster:

\deqn{Z_k = \sum_{j \in C_k} a_j X_j^*}

where \eqn{X_j^*} are the standardized variables and \eqn{a_j} are the
loadings maximizing the explained variance. The standardization is performed
locally within each cluster to ensure comparability.

\strong{Variable assignment (prediction):}

A new variable \eqn{X_{new}} is assigned to the cluster k that maximizes:

\deqn{k^* = \arg\max_k |r(X_{new}^*, Z_k)|}

where \eqn{X_{new}^*} is the standardized new variable and \eqn{r} denotes
the Pearson correlation coefficient.
}

\section{Public Fields}{

\describe{
\item{\code{data}}{Data.frame containing the cleaned input data (quantitative
variables only, no missing values)}
\item{\code{X_last}}{Data.frame of the last variables used in predict().
Used for displaying correlations in summary()}
\item{\code{method}}{Character string of aggregation method (default: "ward.D2")}
\item{\code{dist_method}}{Character string of distance method (always "correlation")}
\item{\code{corr_moy}}{Numeric value of average absolute correlation between
all pairs of variables. Ranges from 0 (no correlation) to 1 (perfect correlation)}
\item{\code{dist_matrix}}{Distance matrix of class "dist" computed from correlations}
\item{\code{hc}}{Hierarchical clustering object of class "hclust". Contains the
dendrogram structure and can be plotted with plot()}
\item{\code{best_k}}{Integer, optimal number of clusters detected automatically
using the elbow method}
\item{\code{clusters}}{Named integer vector of cluster assignments for each
variable. Names are variable names, values are cluster numbers (1 to k)}
\item{\code{predict_result}}{Named integer vector of cluster assignments for
new variables added via predict()}
\item{\code{compo_latent}}{List of latent components, one per cluster. Each
element contains:
\itemize{
\item \code{Zk}: Numeric vector of principal component scores (length = n individuals)
\item \code{vars}: Character vector of variable names in the cluster
\item \code{cor_vals}: Numeric vector of correlations between variables and Zk
\item \code{scaled_data}: Matrix of standardized data for variables in the cluster
}
}
}
}

\examples{
# ═══════════════════════════════════════════════════════════
# Example 1: Basic usage with synthetic data
# ═══════════════════════════════════════════════════════════
set.seed(123)
df <- data.frame(
  var1 = rnorm(30, 10, 2),   # Group 1
  var2 = rnorm(30, 12, 2),   # Group 1 (correlated with var1)
  var3 = rnorm(30, 50, 5),   # Group 2
  var4 = rnorm(30, 52, 5)    # Group 2 (correlated with var3)
)

# Initialize and fit
cah <- CAH$new()
cah$fit(df)

# View optimal k
cat("Optimal number of clusters:", cah$best_k, "\n")

# Partition into clusters
cah$cutree(k = 2)

# View concise results
cah$print()

# View detailed results
cah$summary()

# ═══════════════════════════════════════════════════════════
# Example 2: Adding supplementary variables
# ═══════════════════════════════════════════════════════════

# Create new variables similar to existing ones
new_vars <- data.frame(
  var5 = df$var1 * 0.9 + rnorm(30, 0, 0.5),  # Similar to var1
  var6 = df$var3 * 0.9 + rnorm(30, 0, 1)     # Similar to var3
)

# Assign to existing clusters
cah$predict(new_vars)

# View assignments
print(cah$predict_result)

# ═══════════════════════════════════════════════════════════
# Example 3: Method chaining
# ═══════════════════════════════════════════════════════════
\dontrun{
CAH$new()$fit(df)$cutree()$print()
}

# ═══════════════════════════════════════════════════════════
# Example 4: Working with real data (mtcars)
# ═══════════════════════════════════════════════════════════
\dontrun{
data(mtcars)

# Select numeric variables
df_cars <- mtcars[, c("mpg", "disp", "hp", "drat", "wt", "qsec")]

# Fit and cluster
cah_cars <- CAH$new()
cah_cars$fit(df_cars)
cah_cars$cutree()  # Uses optimal k automatically

# Visualize dendrogram
plot(cah_cars$hc, main = "Vehicle Variables Clustering",
     xlab = "Variables", ylab = "Distance")
rect.hclust(cah_cars$hc, k = cah_cars$best_k, border = "red")

# Identify representative variables
cah_cars$summary()

# Extract cluster memberships
clusters <- cah_cars$clusters
print(clusters)

# Get latent components for further analysis
for (i in seq_along(cah_cars$compo_latent)) {
  comp <- cah_cars$compo_latent[[i]]
  cat("\nCluster", i, ":\n")
  cat("  Variables:", paste(comp$vars, collapse = ", "), "\n")
  cat("  Variance explained:", round(var(comp$Zk), 2), "\n")
}
}

# ═══════════════════════════════════════════════════════════
# Example 5: Handling different data quality issues
# ═══════════════════════════════════════════════════════════
\dontrun{
# Data with missing values
df_na <- data.frame(
  var1 = c(rnorm(25), rep(NA, 5)),
  var2 = rnorm(30),
  var3 = rnorm(30)
)

cah_na <- CAH$new()
cah_na$fit(df_na)  # Will remove rows with NA (with warning)

# Data with constant variables
df_const <- data.frame(
  var1 = rnorm(30),
  var2 = rep(5, 30),  # Constant - will be removed
  var3 = rnorm(30)
)

cah_const <- CAH$new()
cah_const$fit(df_const)  # Will remove constant variable (with warning)

# Data with qualitative variables
df_mixed <- data.frame(
  var1 = rnorm(30),
  var2 = rnorm(30),
  category = factor(rep(c("A", "B", "C"), 10))  # Will be removed
)

cah_mixed <- CAH$new()
cah_mixed$fit(df_mixed)  # Will remove qualitative variable (with warning)
}

# ═══════════════════════════════════════════════════════════
# Example 6: Accessing internal components
# ═══════════════════════════════════════════════════════════
\dontrun{
cah <- CAH$new()
cah$fit(df)
cah$cutree(k = 2)

# Access correlation matrix through distance matrix
dist_mat <- as.matrix(cah$dist_matrix)

# Get correlations from distances: r = 1 - (d²/2)
cor_mat <- 1 - (dist_mat^2 / 2)
print(cor_mat)

# Access dendrogram heights
heights <- cah$hc$height
plot(heights, type = "b", main = "Dendrogram Heights",
     xlab = "Merge step", ylab = "Height")

# Second derivative for elbow detection
d1 <- diff(heights)
d2 <- diff(d1)
plot(d2, type = "b", main = "Second Derivative (Elbow Detection)",
     xlab = "Step", ylab = "Curvature")
abline(v = which.min(d2), col = "red", lty = 2)
}

}
\references{
Rakotomalala, R. (2020). \emph{Classification ascendante hiérarchique}.
Université Lyon 2.
\url{https://eric.univ-lyon2.fr/ricco/cours/cours_classification_hierarchique.html}

Rakotomalala, R. (2020). \emph{Caractérisation des classes en classification automatique}.
Université Lyon 2.

Rakotomalala, R. (2019). \emph{Les classes R6 sous R (Programmation orientée objet sous R)}.
Université Lyon 2.

Rakotomalala, R. \emph{Tanagra - Hierarchical Agglomerative Clustering with PCA}.
\url{http://tutoriels-data-mining.blogspot.com/}

Husson, F., Lê, S., & Pagès, J. (2017). \emph{Exploratory Multivariate Analysis by Example Using R}
(2nd ed.). Chapman and Hall/CRC.
}
\seealso{
\strong{Related R functions:}
\itemize{
\item \code{\link[stats]{hclust}} - Hierarchical clustering
\item \code{\link[stats]{cutree}} - Cut dendrogram into groups
\item \code{\link[stats]{prcomp}} - Principal component analysis
\item \code{\link[stats]{cor}} - Correlation matrix
\item \code{\link[stats]{dist}} - Distance matrix computation
}
}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{data}}{Data.frame containing the cleaned input data}

\item{\code{X_last}}{Data.frame of last variables used in predict()}

\item{\code{method}}{Character string of aggregation method (ward.D2)}

\item{\code{dist_method}}{Distance method for variable clustering}

\item{\code{corr_moy}}{Numeric, average correlation between variables}

\item{\code{hc}}{Hierarchical clustering object (class "hclust")}

\item{\code{best_k}}{Integer, optimal number of clusters}

\item{\code{clusters}}{Named integer vector of cluster assignments}

\item{\code{predict_result}}{Named integer vector of new variable assignments}

\item{\code{dist_matrix}}{Distance matrix (dissimularity)}

\item{\code{compo_latent}}{List of latent components per cluster}

\item{\code{k_current}}{Integer, current k used for cutree}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-CAH-new}{\code{CAH$new()}}
\item \href{#method-CAH-fit}{\code{CAH$fit()}}
\item \href{#method-CAH-cutree}{\code{CAH$cutree()}}
\item \href{#method-CAH-predict}{\code{CAH$predict()}}
\item \href{#method-CAH-plot}{\code{CAH$plot()}}
\item \href{#method-CAH-print}{\code{CAH$print()}}
\item \href{#method-CAH-summary}{\code{CAH$summary()}}
\item \href{#method-CAH-clone}{\code{CAH$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-new"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-new}{}}}
\subsection{Method \code{new()}}{
Create a new CAH object (Constructor)
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$new(method = "ward.D2")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{method}}{Character string specifying the agglomeration method for
hierarchical clustering. Default is "ward.D2"}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
A new \code{CAH} object
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-fit"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-fit}{}}}
\subsection{Method \code{fit()}}{
Fit the hierarchical clustering model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$fit(data)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{data}}{A data.frame or matrix with at least 2 rows and 2 columns}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-cutree"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-cutree}{}}}
\subsection{Method \code{cutree()}}{
Cut the dendrogram to create clusters
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$cutree(k = NULL)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{k}}{Integer, number of clusters. If NULL, uses best_k}
}
\if{html}{\out{</div>}}
}
\subsection{Returns}{
Named integer vector of cluster assignments
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-predict"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-predict}{}}}
\subsection{Method \code{predict()}}{
Predict cluster assignments for new variables
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$predict(X_new)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{X_new}}{A data.frame with the same number of rows as the original data.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-plot"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-plot}{}}}
\subsection{Method \code{plot()}}{
Plot various visualizations of the CAH model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$plot(type = "dendrogramme")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{type}}{Character string specifying plot type}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-print"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-print}{}}}
\subsection{Method \code{print()}}{
Print a summary of the CAH object
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$print(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments (ignored)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-summary"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-summary}{}}}
\subsection{Method \code{summary()}}{
Print a detailed summary of the CAH model
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$summary(...)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{...}}{Additional arguments (ignored)}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CAH-clone"></a>}}
\if{latex}{\out{\hypertarget{method-CAH-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CAH$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
